# ML-Data Folder Structure (Visual Tree)

```
ML-data/
â”‚
â”œâ”€â”€ README.md (ğŸ“– Main documentation - START HERE)
â”‚
â”œâ”€â”€ 1_Raw_Data/                          [Original unprocessed data]
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ All thesis data - labeled.csv   (Original dataset: 1167 rows)
â”‚
â”œâ”€â”€ 2_Processed_Data/                    [Cleaned & analyzed data]
â”‚   â”œâ”€â”€ eda_analysis/                    (Exploratory Data Analysis)
â”‚   â”‚   â”œâ”€â”€ basic_describe.csv
â”‚   â”‚   â”œâ”€â”€ correlation_matrix.csv
â”‚   â”‚   â”œâ”€â”€ data_minmax_scaled.csv
â”‚   â”‚   â”œâ”€â”€ data_standard_scaled.csv
â”‚   â”‚   â”œâ”€â”€ label_counts.csv
â”‚   â”‚   â”œâ”€â”€ mean_median_stats.csv
â”‚   â”‚   â”œâ”€â”€ outliers_count.csv
â”‚   â”‚   â”œâ”€â”€ per_label_means.csv
â”‚   â”‚   â””â”€â”€ tld_label_counts.csv
â”‚   â””â”€â”€ shap_analysis/                   (Model interpretability)
â”‚
â”œâ”€â”€ 3_Scripts/                           [Python scripts for ML pipeline]
â”‚   â”œâ”€â”€ data_preparation/
â”‚   â”‚   â”œâ”€â”€ clean_csv.py                 (Data cleaning)
â”‚   â”‚   â”œâ”€â”€ impute_csv.py                (Missing value handling)
â”‚   â”‚   â””â”€â”€ eda_and_label.py             (EDA & labeling)
â”‚   â”‚
â”‚   â”œâ”€â”€ model_training/
â”‚   â”‚   â”œâ”€â”€ train_models.py              (Train regression models)
â”‚   â”‚   â”œâ”€â”€ train_classifiers.py         (Train classifiers - 3 strategies)
â”‚   â”‚   â””â”€â”€ tune_classifiers.py          (Hyperparameter tuning)
â”‚   â”‚
â”‚   â””â”€â”€ evaluation/
â”‚       â”œâ”€â”€ evaluate_models.py           (Compute metrics)
â”‚       â”œâ”€â”€ visualize_metrics.py         (Generate charts)
â”‚       â”œâ”€â”€ generate_report.py           (Create PDF report)
â”‚       â””â”€â”€ api_predict.py               (Prediction API)
â”‚
â”œâ”€â”€ 4_Trained_Models/                    [Saved ML models]
â”‚   â”œâ”€â”€ README.md
â”‚   â”‚
â”‚   â”œâ”€â”€ regression_models/               (Continuous prediction)
â”‚   â”‚   â”œâ”€â”€ model_rf.joblib              (Random Forest)
â”‚   â”‚   â”œâ”€â”€ model_lgbm.joblib            (LightGBM)
â”‚   â”‚   â”œâ”€â”€ model_keras.h5               (Neural Network)
â”‚   â”‚   â””â”€â”€ scaler.joblib                (Feature scaler)
â”‚   â”‚
â”‚   â””â”€â”€ classification_models/           (Category prediction: Good/Avg/Weak)
â”‚       â”œâ”€â”€ label_tertiles_rf.joblib
â”‚       â”œâ”€â”€ label_tertiles_lgbm.joblib
â”‚       â”œâ”€â”€ label_tertiles_keras.h5
â”‚       â”œâ”€â”€ label_tertiles_scaler.joblib
â”‚       â”œâ”€â”€ label_weighted_rf.joblib
â”‚       â”œâ”€â”€ label_weighted_lgbm.joblib
â”‚       â”œâ”€â”€ label_weighted_keras.h5
â”‚       â”œâ”€â”€ label_weighted_scaler.joblib
â”‚       â”œâ”€â”€ label_kmeans_rf.joblib
â”‚       â”œâ”€â”€ label_kmeans_lgbm.joblib     â­ BEST MODEL (98.47% F1)
â”‚       â”œâ”€â”€ label_kmeans_keras.h5
â”‚       â””â”€â”€ label_kmeans_scaler.joblib
â”‚
â”œâ”€â”€ 5_Results/                           [Performance metrics & analysis]
â”‚   â”œâ”€â”€ README.md
â”‚   â”‚
â”‚   â”œâ”€â”€ metrics/                         (All performance numbers)
â”‚   â”‚   â”œâ”€â”€ evaluation_summary.csv       ğŸ“Š MAIN METRICS FILE
â”‚   â”‚   â”œâ”€â”€ classification_summary.json
â”‚   â”‚   â”œâ”€â”€ best_models_per_strategy.json
â”‚   â”‚   â”œâ”€â”€ accuracy/
â”‚   â”‚   â”‚   â””â”€â”€ evaluation_sorted_by_accuracy.csv
â”‚   â”‚   â”œâ”€â”€ precision_macro/
â”‚   â”‚   â”‚   â””â”€â”€ evaluation_sorted_by_precision_macro.csv
â”‚   â”‚   â”œâ”€â”€ recall_macro/
â”‚   â”‚   â”‚   â””â”€â”€ evaluation_sorted_by_recall_macro.csv
â”‚   â”‚   â””â”€â”€ f1_macro/
â”‚   â”‚       â””â”€â”€ evaluation_sorted_by_f1_macro.csv
â”‚   â”‚
â”‚   â”œâ”€â”€ reports/                         (Training details)
â”‚   â”‚   â”œâ”€â”€ training_summary.json
â”‚   â”‚   â”œâ”€â”€ feature_importances.csv
â”‚   â”‚   â”œâ”€â”€ keras_history.json
â”‚   â”‚   â””â”€â”€ report.pdf
â”‚   â”‚
â”‚   â””â”€â”€ confusion_matrices/              (Prediction accuracy breakdowns)
â”‚       â”œâ”€â”€ confusion_label_tertiles_rf.png
â”‚       â”œâ”€â”€ confusion_label_tertiles_lgbm.png
â”‚       â”œâ”€â”€ confusion_label_tertiles_keras.png
â”‚       â”œâ”€â”€ confusion_label_weighted_rf.png
â”‚       â”œâ”€â”€ confusion_label_weighted_lgbm.png
â”‚       â”œâ”€â”€ confusion_label_weighted_keras.png
â”‚       â”œâ”€â”€ confusion_label_kmeans_rf.png
â”‚       â”œâ”€â”€ confusion_label_kmeans_lgbm.png
â”‚       â””â”€â”€ confusion_label_kmeans_keras.png
â”‚
â”œâ”€â”€ 6_Visualizations/                    [Charts & graphs] ğŸ“ˆ
â”‚   â”œâ”€â”€ README.md
â”‚   â”‚
â”‚   â”œâ”€â”€ accuracy_comparison.png          (Compare by accuracy)
â”‚   â”œâ”€â”€ precision_macro_comparison.png   (Compare by precision)
â”‚   â”œâ”€â”€ recall_macro_comparison.png      (Compare by recall)
â”‚   â”œâ”€â”€ f1_macro_comparison.png          (Compare by F1-score)
â”‚   â”‚
â”‚   â”œâ”€â”€ accuracy_individual_bars.png     (Detailed accuracy ranking)
â”‚   â”œâ”€â”€ precision_macro_individual_bars.png
â”‚   â”œâ”€â”€ recall_macro_individual_bars.png
â”‚   â”œâ”€â”€ f1_macro_individual_bars.png
â”‚   â”‚
â”‚   â”œâ”€â”€ all_metrics_heatmap.png          ğŸ”¥ COMPREHENSIVE HEATMAP
â”‚   â”œâ”€â”€ model_comparison_radar.png       ğŸ¯ RADAR CHART
â”‚   â”œâ”€â”€ performance_summary_report.txt   ğŸ“ TEXT SUMMARY
â”‚   â”‚
â”‚   â””â”€â”€ models/                          (Model-specific data)
â”‚       â”œâ”€â”€ RandomForest/
â”‚       â”‚   â”œâ”€â”€ RandomForest_all_metrics.csv
â”‚       â”‚   â””â”€â”€ RandomForest_summary.json
â”‚       â””â”€â”€ LightGBM/
â”‚           â”œâ”€â”€ LightGBM_all_metrics.csv
â”‚           â””â”€â”€ LightGBM_summary.json
â”‚
â”œâ”€â”€ 7_Documentation/                     [Reference materials]
â”‚   â”œâ”€â”€ MetricsData.ipynb                (Jupyter notebook)
â”‚   â””â”€â”€ original_README.md               (Original setup guide)
â”‚
â””â”€â”€ Code/                                [ORIGINAL BACKUP - All files preserved]
    â””â”€â”€ (Original unorganized structure - kept for reference)
```

---

## ğŸ¯ Quick Navigation Guide

| What You Need | Where to Look |
|---------------|---------------|
| **See all model performance** | `5_Results/metrics/evaluation_summary.csv` |
| **Best model info** | `6_Visualizations/models/LightGBM/LightGBM_summary.json` |
| **Charts for presentation** | `6_Visualizations/*.png` |
| **Confusion matrices** | `5_Results/confusion_matrices/` |
| **Load a trained model** | `4_Trained_Models/classification_models/label_kmeans_lgbm.joblib` |
| **Run training** | `3_Scripts/model_training/train_classifiers.py` |
| **Generate new charts** | `3_Scripts/evaluation/visualize_metrics.py` |
| **Original data** | `1_Raw_Data/All thesis data - labeled.csv` |

---

## ğŸ† Best Results Quick Reference

**Top Performer: LightGBM + K-means Labeling**
- Location: `4_Trained_Models/classification_models/label_kmeans_lgbm.joblib`
- Accuracy: 97.86%
- Precision: 98.40%
- Recall: 98.53%
- F1-Score: 98.47%

**Summary:** `6_Visualizations/models/LightGBM/LightGBM_summary.json`

---

**Legend:**
ğŸ“– Documentation | ğŸ“Š Data | ğŸ“ˆ Charts | ğŸ”¥ Important | â­ Best | ğŸ¯ Key File
